{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "simple clean working image captioning.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "q-2wTuktfkrg"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTrMiDYtfoeh",
        "colab_type": "text"
      },
      "source": [
        "# **IMPORT & DEFINE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THzwSdAfdUZ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture \n",
        "\n",
        "from tensorflow import keras \n",
        "import keras.layers as L \n",
        "import keras.backend as K\n",
        "from keras.optimizers import Adam \n",
        "from keras.optimizers.schedules import ExponentialDecay\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "import numpy as np \n",
        "import cv2 \n",
        "import pickle \n",
        "import random \n",
        "\n",
        "import re\n",
        "import nltk\n",
        "nltk.download(\"popular\")\n",
        "from nltk.corpus import stopwords \n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "PAD = \"#PAD#\"\n",
        "UNK = \"#UNK#\" \n",
        "START = \"#START#\" \n",
        "END = \"#END#\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-2wTuktfkrg",
        "colab_type": "text"
      },
      "source": [
        "# **FUNCTIONS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0AylNfwfV0d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def read_pickle(pickle_file):\n",
        "  with open(pickle_file, 'rb') as f :\n",
        "    return pickle.load(f)\n",
        "\n",
        "def show_image(image):\n",
        "  from google.colab.patches import cv2_imshow\n",
        "  cv2_imshow(image)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def seq_generator(image_embeddings, captions, SENTENCE_LEN, padding_element, NUM_WORDS, batch_size):\n",
        "\n",
        "  def pad_sequence(seq, seq_len, padding_element):\n",
        "    if len(seq) >= seq_len: return seq[0:seq_len] \n",
        "    else: return seq + [padding_element]*(seq_len-len(seq))\n",
        "\n",
        "  x1, x2, y = [], [], [] \n",
        "  n = 0\n",
        "\n",
        "  while True :\n",
        "    for i, caption in enumerate(captions):\n",
        "      n += 1 \n",
        "      seq = random.choice(caption)\n",
        "      for j in range(len(seq)):\n",
        "        inseq, outseq = seq[:j], seq[j] \n",
        "        x1.append(image_embeddings[i])\n",
        "        x2.append(pad_sequence(inseq, SENTENCE_LEN, padding_element))\n",
        "        y.append(to_categorical([outseq], NUM_WORDS)[0] )\n",
        "      if n == batch_size :\n",
        "        yield ([np.array(x1), np.array(x2)], np.array(y))\n",
        "        x1, x2, y = [], [], [] \n",
        "        n = 0\n",
        "\n",
        "def get_num_sentences(captions):\n",
        "  return len(captions) * 5\n",
        "\n",
        "def split_sentence(sentence):\n",
        "  return list(filter(lambda x : len(x) > 2 and x not in stop_words, re.split('\\W+', sentence.lower())))\n",
        "\n",
        "def get_max_len(captions):\n",
        "  maxlength = 0 \n",
        "  for caption in captions:\n",
        "    for sentence in caption:\n",
        "      maxlength = max(maxlength, len(sentence))\n",
        "  return maxlength\n",
        "\n",
        "def get_vocab(captions, vocab_size):\n",
        "  vocab = dict()\n",
        "  for caption in captions :\n",
        "    for sentence in caption:\n",
        "      for word in split_sentence(sentence):\n",
        "        if len(word) < 3 or word in stop_words: continue \n",
        "        if word[-1] == '.' : word = word[:-1]\n",
        "        vocab[word] = vocab.get(word, 0) + 1\n",
        "\n",
        "  vocab = [word[0] for word in sorted(vocab.items(), key = lambda item : item[1], reverse = True)][0: vocab_size-4] + [PAD, UNK, START, END]\n",
        "  word2ix = {word:index for index, word in enumerate(vocab)} \n",
        "  ix2word = {index:word for index, word in enumerate(vocab)} \n",
        "  return vocab, word2ix, ix2word\n",
        "\n",
        "def captions2captions(captions, word2ix):\n",
        "  new_captions = [] \n",
        "  for caption in captions : \n",
        "    new_caption = [] \n",
        "    for sentence in caption:\n",
        "      #there's a problem here, if a word ends with '.' it will be ignored while we ideally want to just erase the '.' and get the word\n",
        "      new_caption.append([word2ix[START]] + [word2ix[word] for word in split_sentence(sentence) if word in word2ix] + [word2ix[END]])\n",
        "    new_captions.append(new_caption)\n",
        "  return new_captions\n",
        "\n",
        "\n",
        "def build_model(SENTENCE_LEN, NUM_WORDS, WORD_EMBED_SIZE, LSTM_UNITS):\n",
        "  image_features = L.Input(shape = (2048,)) \n",
        "  densed_IF = L.Dense(256)(image_features) \n",
        "  densed_IF = L.Dense(WORD_EMBED_SIZE)(densed_IF)\n",
        "  initial_state = [densed_IF, densed_IF]\n",
        "\n",
        "  words = L.Input(shape = (SENTENCE_LEN,))\n",
        "  words_embeddings = L.Embedding(NUM_WORDS, WORD_EMBED_SIZE)(words) \n",
        "\n",
        "  lstm_output = L.LSTM(LSTM_UNITS)(inputs = words_embeddings, initial_state = initial_state) \n",
        "\n",
        "  decoded_words1 = L.Dense(512, activation = 'relu')(lstm_output) \n",
        "  decoded_words2 = L.Dense(1024, activation = 'relu')(decoded_words1)\n",
        "  final_output = L.Dense(NUM_WORDS, activation = 'softmax')(decoded_words2) \n",
        "\n",
        "  model = keras.Model(inputs = [image_features, words], outputs = final_output) \n",
        "  return model\n",
        "\n",
        "def download_data():\n",
        "\n",
        "  !wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1-5AhtN3za59P6WsHBhVRw7Lonx4wk6xu' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1-5AhtN3za59P6WsHBhVRw7Lonx4wk6xu\" -O train_image_features.pickle && rm -rf /tmp/cookies.txt\n",
        "  train_image_embeds = read_pickle('train_image_features.pickle') \n",
        "\n",
        "  !wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1--FpSwNWO9X8l5YneuTHcs--EtJlJZOP' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1--FpSwNWO9X8l5YneuTHcs--EtJlJZOP\" -O val_image_embeds.pickle && rm -rf /tmp/cookies.txt\n",
        "  val_image_embeds = read_pickle('val_image_embeds.pickle') \n",
        "\n",
        "  !wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1-729Stj7PWEztvH-YVJ_nivo4QmGR-ro' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1-729Stj7PWEztvH-YVJ_nivo4QmGR-ro\" -O train_captions.pickle && rm -rf /tmp/cookies.txt\n",
        "  train_captions = read_pickle('train_captions.pickle') \n",
        "\n",
        "  !wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1-1WfbEjN052jaHSUb4h5J_t9BIgziSP3' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1-1WfbEjN052jaHSUb4h5J_t9BIgziSP3\" -O val_captions.pickle && rm -rf /tmp/cookies.txt\n",
        "  val_captions = read_pickle('val_captions.pickle') \n",
        "\n",
        "  !wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1-77wJhnWLCnvmBOvOFuQrAv_ekjE-ooa' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1-77wJhnWLCnvmBOvOFuQrAv_ekjE-ooa\" -O train_image_fns.pickle && rm -rf /tmp/cookies.txt\n",
        "  train_image_fns = read_pickle('train_image_fns.pickle') \n",
        "\n",
        "  !wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1-2HLbOGLT4E9V_IywjwNWBviyz49QjrY' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1-2HLbOGLT4E9V_IywjwNWBviyz49QjrY\" -O val_image_fns.pickle && rm -rf /tmp/cookies.txt\n",
        "  val_image_fns = read_pickle('val_image_fns.pickle') \n",
        "\n",
        "  return train_image_embeds, val_image_embeds, train_captions, val_captions, train_image_fns, val_image_fns \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-RYEd5Lrf6m",
        "colab_type": "text"
      },
      "source": [
        "# **CAPTIONS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJCJWlzNfkOY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture \n",
        "train_image_embeds, val_image_embeds, train_captions, val_captions, train_image_fns, val_image_fns = download_data() \n",
        "all_captions = train_captions + val_captions "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAmkKJh2gMa1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NUM_WORDS = 5000\n",
        "NUM_SENTENCES = get_num_sentences(train_captions) \n",
        "SENTENCE_LEN = 20\n",
        "WORD_EMBED_SIZE = 200\n",
        "\n",
        "VOCAB, word2ix, ix2word = get_vocab(all_captions, vocab_size = NUM_WORDS) \n",
        "train_captions_ix = captions2captions(train_captions, word2ix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEC57LPUOrfU",
        "colab_type": "text"
      },
      "source": [
        "# **MODEL**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDDkjW2ajXpC",
        "colab_type": "text"
      },
      "source": [
        "## TRAIN & SAVE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNyUTOrBv09b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "K.clear_session() \n",
        "model = build_model(SENTENCE_LEN, NUM_WORDS, WORD_EMBED_SIZE, LSTM_UNITS = 200) \n",
        "\n",
        "scheduler = ExponentialDecay(initial_learning_rate= 1e-3, decay_rate = 0.93, decay_steps = 10000)\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = Adam(scheduler))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPVXyhT-1QMt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "outputId": "d0ccef46-30e3-4e17-91e0-184432945946"
      },
      "source": [
        "epochs = 10\n",
        "BS = 50\n",
        "steps = NUM_SENTENCES // BS\n",
        "train_generator = seq_generator(train_image_embeds, train_captions_ix, SENTENCE_LEN, word2ix[PAD], NUM_WORDS, batch_size = BS)\n",
        "\n",
        "for epoch in range(epochs) :\n",
        "  model.fit(train_generator, epochs = 1, steps_per_epoch = steps, verbose = 1) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8278/8278 [==============================] - 246s 30ms/step - loss: 3.7889\n",
            "8278/8278 [==============================] - 240s 29ms/step - loss: 3.0566\n",
            "8278/8278 [==============================] - 238s 29ms/step - loss: 2.9034\n",
            "8278/8278 [==============================] - 240s 29ms/step - loss: 2.8114\n",
            "8278/8278 [==============================] - 237s 29ms/step - loss: 2.7491\n",
            "8278/8278 [==============================] - 236s 29ms/step - loss: 2.6986\n",
            "8278/8278 [==============================] - 237s 29ms/step - loss: 2.6576\n",
            "8278/8278 [==============================] - 237s 29ms/step - loss: 2.6211\n",
            "8278/8278 [==============================] - 236s 29ms/step - loss: 2.5885\n",
            "8278/8278 [==============================] - 235s 28ms/step - loss: 2.5622\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fNDTOrGNwWg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_path = '/content/drive/My Drive/image_caption_project/partial_model.h5'\n",
        "model.save_weights(model_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76VYejYyjcoa",
        "colab_type": "text"
      },
      "source": [
        "## LOAD & PLAY"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6IjoDz1NxXF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_path = '/content/drive/My Drive/image_caption_project/partial_model.h5'\n",
        "\n",
        "K.clear_session() \n",
        "model = build_model(SENTENCE_LEN, NUM_WORDS, WORD_EMBED_SIZE, LSTM_UNITS = 200) \n",
        "model.load_weights(model_path) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DplqZIQTjtK5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}